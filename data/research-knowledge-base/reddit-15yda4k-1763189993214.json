{
  "finding": {
    "findingId": "reddit-15yda4k-1763189993214",
    "source": "reddit",
    "category": "model-quantization",
    "title": "[R] QuIP: 2-Bit Quantization of Large Language Models With Guarantees - Cornell University 2023",
    "description": "Paper: [https://arxiv.org/abs/2307.13304](https://arxiv.org/abs/2307.13304)\n\nGithub: [https://github.com/jerry-chee/QuIP](https://github.com/jerry-chee/QuIP)\n\nAbstract:\n\n&gt;This work studies post-training parameter quantization in large language models (LLMs). We introduce quantization with incoherence processing (QuIP), a new method based on the insight that quantization benefits from incoherent weight and Hessian matrices, i.e., from the weights and the directions in which it is important to ",
    "data": {
      "query": "model quantization methods",
      "category": "model-quantization",
      "subreddit": "MachineLearning",
      "score": 47,
      "numComments": 9,
      "author": "Singularian2501",
      "createdUtc": 1692726944,
      "sources": [
        "reddit"
      ]
    },
    "valuePotential": 0.05,
    "implementationSuggestion": "Explore model quantization methods for efficiency gains",
    "url": "https://reddit.com/r/MachineLearning/comments/15yda4k/r_quip_2bit_quantization_of_large_language_models/",
    "tags": [
      "model-quantization",
      "quantization",
      "model"
    ],
    "timestamp": "2025-11-15T06:59:53.214Z"
  },
  "storedAt": "2025-11-15T06:59:53.412Z",
  "accessedCount": 0,
  "lastAccessed": "2025-11-15T06:59:53.412Z",
  "implementationStatus": "pending"
}