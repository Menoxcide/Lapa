{
  "finding": {
    "findingId": "reddit-1ienetu-1763193593070",
    "source": "reddit",
    "category": "nim-inference",
    "title": "Deepseek R1 is now hosted by Nvidia",
    "description": "NVIDIA just brought DeepSeek-R1 671-bn param model  to NVIDIA NIM microservice on build.nvidia .com\n\n- The DeepSeek-R1 NIM microservice can deliver up to 3,872 tokens per second on a single NVIDIA HGX H200 system.\n\n- Using NVIDIA Hopper architecture, DeepSeek-R1 can deliver high-speed inference by leveraging FP8 Transformer Engines and 900 GB/s NVLink bandwidth for expert communication.\n\n- As usual with NVIDIA's NIM, its a enterprise-scale setu to securely experiment, and deploy AI agents with i",
    "data": {
      "query": "NVIDIA NIM inference microservice",
      "category": "nim-inference",
      "subreddit": "LocalLLaMA",
      "score": 680,
      "numComments": 56,
      "author": "Outrageous-Win-3244",
      "createdUtc": 1738352684,
      "sources": [
        "reddit"
      ]
    },
    "valuePotential": 0.15000000000000002,
    "implementationSuggestion": "Evaluate NVIDIA NIM integration opportunities for inference optimization",
    "url": "https://reddit.com/r/LocalLLaMA/comments/1ienetu/deepseek_r1_is_now_hosted_by_nvidia/",
    "tags": [
      "nim-inference",
      "inference",
      "nim"
    ],
    "timestamp": "2025-11-15T07:59:53.070Z"
  },
  "storedAt": "2025-11-15T07:59:53.383Z",
  "accessedCount": 0,
  "lastAccessed": "2025-11-15T07:59:53.383Z",
  "implementationStatus": "pending"
}