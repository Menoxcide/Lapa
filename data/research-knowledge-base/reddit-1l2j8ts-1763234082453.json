{
  "finding": {
    "findingId": "reddit-1l2j8ts-1763234082453",
    "source": "reddit",
    "category": "model-quantization",
    "title": "[R] GuidedQuant: Boost layer-wise PTQ methods using the end loss guidance (Qwen3, Gemma3, Llama3.3 / 2~4bit quantization) (ICML 2025)",
    "description": "**Paper (ICML 2025):** [https://arxiv.org/abs/2505.07004](https://arxiv.org/abs/2505.07004)\n\n**Code:** [https://github.com/snu-mllab/GuidedQuant](https://github.com/snu-mllab/GuidedQuant)\n\n**HuggingFace Collection:** 2\\~4-bit quantized Qwen3-32B, gemma-3-27b-it, Llama-3.1-8B-Instruct, Llama-3.3-70B-Instruct  → [Link](https://huggingface.co/collections/jusjinuk/instruction-tuned-models-guidedquant-68334269c44cd3eb21f7bd61)\n\n**TL;DR:** **GuidedQuant** boosts layer-wise PTQ methods by integrating e",
    "data": {
      "query": "model quantization methods",
      "category": "model-quantization",
      "subreddit": "MachineLearning",
      "score": 13,
      "numComments": 0,
      "author": "jusjinuk",
      "createdUtc": 1748974620,
      "sources": [
        "reddit"
      ]
    },
    "valuePotential": 0.05,
    "implementationSuggestion": "Explore model quantization methods for efficiency gains",
    "url": "https://reddit.com/r/MachineLearning/comments/1l2j8ts/r_guidedquant_boost_layerwise_ptq_methods_using/",
    "tags": [
      "model-quantization",
      "quantization",
      "model"
    ],
    "timestamp": "2025-11-15T19:14:42.453Z"
  },
  "storedAt": "2025-11-15T19:14:42.663Z",
  "accessedCount": 0,
  "lastAccessed": "2025-11-15T19:14:42.663Z",
  "implementationStatus": "pending"
}