{
  "finding": {
    "findingId": "reddit-1mqh2e1-1763234082199",
    "source": "reddit",
    "category": "nim-inference",
    "title": "Need help with Nvidia tooling to run gpt-oss 20b on RTX5090",
    "description": "### My rig: \n- RTX 5090\n- 64 GB RAM\n- AMD 9950X\n\nIâ€™m trying to run `openai/gpt-oss-20b` in a \"native\" setup, without quantization or repacking to GGUF. Just the \"raw safetensors\".\n\n### NVIDIA suggests the following options:\n#### 1. NIM\n- Documentation: `https://build.nvidia.com/openai/gpt-oss-20b/deploy`\n- Docker container: `nvcr.io/nim/openai/gpt-oss-20b:latest`\n\n#### 2. TensorRT-LLM\n- Documentation: `https://catalog.ngc.nvidia.com/orgs/nvidia/teams/tensorrt-llm/containers/release`\n- Docker con",
    "data": {
      "query": "NVIDIA NIM inference microservice",
      "category": "nim-inference",
      "subreddit": "LocalLLaMA",
      "score": 13,
      "numComments": 30,
      "author": "vanbukin",
      "createdUtc": 1755214465,
      "sources": [
        "reddit"
      ]
    },
    "valuePotential": 0.05,
    "implementationSuggestion": "Evaluate NVIDIA NIM integration opportunities for inference optimization",
    "url": "https://reddit.com/r/LocalLLaMA/comments/1mqh2e1/need_help_with_nvidia_tooling_to_run_gptoss_20b/",
    "tags": [
      "nim-inference",
      "inference",
      "nim"
    ],
    "timestamp": "2025-11-15T19:14:42.199Z"
  },
  "storedAt": "2025-11-15T19:14:42.510Z",
  "accessedCount": 0,
  "lastAccessed": "2025-11-15T19:14:42.510Z",
  "implementationStatus": "pending"
}